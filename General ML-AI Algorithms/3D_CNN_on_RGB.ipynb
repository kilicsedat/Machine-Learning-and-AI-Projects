{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAtb0Cb-7m8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2080beac-e58c-402b-cf5b-22800b698745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting TensorRT\n",
            "  Downloading tensorrt-8.6.0-cp39-none-manylinux_2_17_x86_64.whl (819.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.2/819.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.55-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.5/823.5 KB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12\n",
            "  Downloading nvidia_cudnn_cu12-8.8.1.3-py3-none-manylinux1_x86_64.whl (718.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.4/718.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12\n",
            "  Downloading nvidia_cublas_cu12-12.1.0.26-py3-none-manylinux1_x86_64.whl (379.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m379.5/379.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cuda-runtime-cu12, nvidia-cublas-cu12, nvidia-cudnn-cu12, TensorRT\n",
            "Successfully installed TensorRT-8.6.0 nvidia-cublas-cu12-12.1.0.26 nvidia-cuda-runtime-cu12-12.1.55 nvidia-cudnn-cu12-8.8.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install TensorRT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FrSINmbFJuy",
        "outputId": "7fdb5c4c-9e20-4fbc-8c29-fa5eb7a45003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.9.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.9/dist-packages (from mediapipe) (4.7.0.72)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from mediapipe) (1.22.4)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.9/dist-packages (from mediapipe) (22.2.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from mediapipe) (23.3.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.9/dist-packages (from mediapipe) (3.19.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (23.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (1.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (4.39.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (8.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (0.11.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (5.12.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->mediapipe) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.15.0)\n",
            "Installing collected packages: mediapipe\n",
            "Successfully installed mediapipe-0.9.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 KB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from tensorboardX) (1.22.4)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorboardX) (3.19.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorboardX) (23.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6\n"
          ]
        }
      ],
      "source": [
        "!pip install mediapipe\n",
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylT4BBPtEpxR",
        "outputId": "8a9cef8f-63ea-4238-e847-fb20bcece906"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxiyZoOg9fG-",
        "outputId": "4dd45b56-6d71-4645-ce6e-9e1ba30f32d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/sample data/ana_data\n"
          ]
        }
      ],
      "source": [
        "cd /content/gdrive/MyDrive/sample data/ana_data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TH_YDjvg96Ag",
        "outputId": "30b2a989-315e-48e4-b1f1-af604bf4e225"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datalo_3.py  \u001b[0m\u001b[01;34mdata_preprocessed2\u001b[0m/        \u001b[01;34mshortened_data_for_loader\u001b[0m/\n",
            "datalo_6.py  shoplifting_train_lab.txt\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_9AXb7D4DSz",
        "outputId": "a9fb927d-852c-4f2e-fb3f-6ab4628d172e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-20 23:52:56.365384: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-20 23:52:58.699184: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-03-20 23:52:58.699331: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-03-20 23:52:58.699356: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Device being used: cuda:0\n",
            "Training C3D from scratch...\n",
            "Total params: 763.28M\n",
            "Training model on shortened_data_for_loader dataset...\n",
            "Number of train videos: 64\n",
            "Number of val videos: 16\n",
            "Number of test videos: 20\n",
            "  0% 0/8 [00:00<?, ?it/s]Downloading model to /usr/local/lib/python3.9/dist-packages/mediapipe/modules/pose_landmark/pose_landmark_heavy.tflite\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python datalo_6.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "id": "99XB2g9m5YY4",
        "outputId": "35f0c14d-9fc4-420e-abc9-9d5d09dff922"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/MyDrive/sample data/ana_data'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5Xf6OXjATA3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import mediapipe as mp\n",
        "\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "kwPZOCldHDq5",
        "outputId": "9cac0cde-dbb6-4078-b532-2453042aa6e8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nmp_pose = mp.solutions.pose\\nmp_drawing = mp.solutions.drawing_utils \\nmp_drawing_styles = mp.solutions.drawing_styles\\n\\ndef detectP(frame):\\n    with mp_pose.Pose(\\n        static_image_mode=True, min_detection_confidence=0.5, model_complexity=2) as pose:\\n      \\n        # Convert the BGR image to RGB and process it with MediaPipe Pose.\\n        results = pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\\n        \\n        # Print nose landmark.\\n        image_hight, image_width, _ = frame.shape\\n        if not results.pose_landmarks:\\n          a =1\\n\\n        # Draw pose landmarks.\\n        annotated_image = frame.copy()\\n        mp_drawing.draw_landmarks(\\n            annotated_image,\\n            results.pose_landmarks,\\n            mp_pose.POSE_CONNECTIONS,\\n            landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\\n                                                                    \\n    return annotated_image\\n    '"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "mp_pose = mp.solutions.pose\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "\n",
        "def detectP(frame):\n",
        "    with mp_pose.Pose(\n",
        "        static_image_mode=True, min_detection_confidence=0.5, model_complexity=2) as pose:\n",
        "\n",
        "        # Convert the BGR image to RGB and process it with MediaPipe Pose.\n",
        "        results = pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        # Print nose landmark.\n",
        "        image_hight, image_width, _ = frame.shape\n",
        "        if not results.pose_landmarks:\n",
        "          a =1\n",
        "\n",
        "        # Draw pose landmarks.\n",
        "        annotated_image = frame.copy()\n",
        "        mp_drawing.draw_landmarks(\n",
        "            annotated_image,\n",
        "            results.pose_landmarks,\n",
        "            mp_pose.POSE_CONNECTIONS,\n",
        "            landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
        "\n",
        "    return annotated_image\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLh8TcCiAJ-c"
      },
      "outputs": [],
      "source": [
        "\n",
        "class VideoDataset(Dataset):\n",
        "    \"\"\"A Dataset for a folder of videos. Expects the directory structure to be\n",
        "    directory->[train/val/test]->[class labels]->[videos]. Initializes with a list\n",
        "    of all file names, along with an array of labels, with label being automatically\n",
        "    inferred from the respective folder names.\n",
        "\n",
        "        Args:\n",
        "            dataset (str): Name of dataset. Defaults to 'ucf101'.\n",
        "            split (str): Determines which folder of the directory the dataset will read from. Defaults to 'train'.\n",
        "            clip_len (int): Determines how many frames are there in each clip. Defaults to 16.\n",
        "            preprocess (bool): Determines whether to preprocess dataset. Default is False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset='/content/gdrive/MyDrive/sample data/ana_data/shortened_data_for_loader', split='train', clip_len=32, preprocess=False):\n",
        "        self.root_dir, self.output_dir = dataset, '/content/gdrive/MyDrive/sample data/ana_data/data_preprocessed2'\n",
        "        folder = os.path.join(self.output_dir, split)\n",
        "        self.clip_len = clip_len\n",
        "        self.split = split\n",
        "        self.resize_height = 240\n",
        "        self.resize_width = 320\n",
        "        self.crop_size = 112\n",
        "\n",
        "        if not self.check_integrity():\n",
        "            raise RuntimeError('Dataset not found or corrupted.' +\n",
        "                               ' You need to download it from official website.')\n",
        "\n",
        "        if  preprocess:\n",
        "            print('Preprocessing of {} dataset, this will take long, but it will be done only once.'.format(dataset))\n",
        "            self.preprocess()\n",
        "\n",
        "        # Obtain all the filenames of files inside all the class folders\n",
        "        # Going through each class folder one at a time\n",
        "        self.fnames, labels = [], []\n",
        "        for label in sorted(os.listdir(folder)):\n",
        "            for fname in os.listdir(os.path.join(folder, label)):\n",
        "                self.fnames.append(os.path.join(folder, label, fname))\n",
        "                labels.append(label)\n",
        "\n",
        "        assert len(labels) == len(self.fnames)\n",
        "        print('Number of {} videos: {:d}'.format(split, len(self.fnames)))\n",
        "\n",
        "        # Prepare a mapping between the label names (strings) and indices (ints)\n",
        "        self.label2index = {label: index for index, label in enumerate(sorted(set(labels)))}\n",
        "        # Convert the list of label names into an array of label indices\n",
        "        self.label_array = np.array([self.label2index[label] for label in labels], dtype=int)\n",
        "\n",
        "        if dataset == '/content/gdrive/MyDrive/sample data/ana_data/shortened_data_for_loader':\n",
        "            if not os.path.exists('/content/gdrive/MyDrive/sample data/ana_data/shoplifting_train_lab.txt'):\n",
        "                with open('/content/gdrive/MyDrive/sample data/ana_data/shoplifting_train_lab.txt', 'w') as f:\n",
        "                    for id, label in enumerate(sorted(self.label2index)):\n",
        "                        f.writelines(str(id+1) + ' ' + label + '\\n')\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.fnames)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Loading and preprocessing.\n",
        "        buffer = self.load_frames(self.fnames[index])\n",
        "        buffer = self.crop(buffer, self.clip_len, self.crop_size)\n",
        "        labels = np.array(self.label_array[index])\n",
        "\n",
        "        if self.split == 'test':\n",
        "            # Perform data augmentation\n",
        "            buffer = self.randomflip(buffer)\n",
        "        buffer = self.normalize(buffer)\n",
        "        buffer = self.to_tensor(buffer)\n",
        "        buffer = torch.from_numpy(buffer)\n",
        "\n",
        "        return buffer, labels\n",
        "\n",
        "\n",
        "\n",
        "    def check_integrity(self):\n",
        "        if not os.path.exists(self.root_dir):\n",
        "            return False\n",
        "        else:\n",
        "            return True\n",
        "    '''\n",
        "    def check_preprocess(self):\n",
        "        # TODO: Check image size in output_dir\n",
        "        if not os.path.exists(self.output_dir):\n",
        "            return False\n",
        "        elif not os.path.exists(os.path.join(self.output_dir, 'shoplifting_train')):\n",
        "            return False\n",
        "\n",
        "        for ii, video_class in enumerate(os.listdir(os.path.join(self.output_dir, 'shoplifting_train'))):\n",
        "            if video_class == '.DS_Store':\n",
        "              continue\n",
        "            for video in os.listdir(os.path.join(self.output_dir, 'shoplifting_train', video_class)):\n",
        "                video_name = os.path.join(self.output_dir, 'shoplifting_train', video_class, video)\n",
        "\n",
        "                image = cv2.imread(video_name)\n",
        "                if np.shape(image)[0] != 320 or np.shape(image)[1] != 240:\n",
        "                    return False\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            if ii == 10:\n",
        "                break\n",
        "\n",
        "        return True\n",
        "    '''\n",
        "    def preprocess(self):\n",
        "        if not os.path.exists(self.output_dir):\n",
        "            os.mkdir(self.output_dir)\n",
        "            os.mkdir(os.path.join(self.output_dir, 'train'))\n",
        "            os.mkdir(os.path.join(self.output_dir, 'val'))\n",
        "            os.mkdir(os.path.join(self.output_dir, 'test'))\n",
        "\n",
        "        # Split train/val/test sets\n",
        "        for file in os.listdir(self.root_dir):\n",
        "            file_path = os.path.join(self.root_dir, file)\n",
        "            video_files = [name for name in os.listdir(file_path)]\n",
        "\n",
        "            train_and_valid, test = train_test_split(video_files, test_size=0.2, random_state=42)\n",
        "            train, val = train_test_split(train_and_valid, test_size=0.2, random_state=42)\n",
        "\n",
        "            train_dir = os.path.join(self.output_dir, 'train', file)\n",
        "            val_dir = os.path.join(self.output_dir, 'val', file)\n",
        "            test_dir = os.path.join(self.output_dir, 'test', file)\n",
        "\n",
        "            if not os.path.exists(train_dir):\n",
        "                os.mkdir(train_dir)\n",
        "            if not os.path.exists(val_dir):\n",
        "                os.mkdir(val_dir)\n",
        "            if not os.path.exists(test_dir):\n",
        "                os.mkdir(test_dir)\n",
        "\n",
        "            for video in train:\n",
        "                self.process_video(video, file, train_dir)\n",
        "\n",
        "            for video in val:\n",
        "                self.process_video(video, file, val_dir)\n",
        "\n",
        "            for video in test:\n",
        "                self.process_video(video, file, test_dir)\n",
        "\n",
        "        print('Preprocessing finished.')\n",
        "\n",
        "\n",
        "\n",
        "    def process_video(self, video, action_name, save_dir):\n",
        "        # Initialize a VideoCapture object to read video data into a numpy array\n",
        "        video_filename = video.split('.')[0]\n",
        "        print(video_filename)\n",
        "        if not os.path.exists(os.path.join(save_dir, video_filename)):\n",
        "            os.mkdir(os.path.join(save_dir, video_filename))\n",
        "\n",
        "        capture = cv2.VideoCapture(os.path.join(self.root_dir, action_name, video))\n",
        "\n",
        "        frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        frame_width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        frame_height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "        # Make sure splited video has at least 16 frames\n",
        "        EXTRACT_FREQUENCY = 4\n",
        "        if frame_count // EXTRACT_FREQUENCY <= 16:\n",
        "            EXTRACT_FREQUENCY -= 1\n",
        "            if frame_count // EXTRACT_FREQUENCY <= 16:\n",
        "                EXTRACT_FREQUENCY -= 1\n",
        "                if frame_count // EXTRACT_FREQUENCY <= 16:\n",
        "                    EXTRACT_FREQUENCY -= 1\n",
        "\n",
        "        count = 0\n",
        "        i = 0\n",
        "        retaining = True\n",
        "\n",
        "        while (count < frame_count and retaining):\n",
        "            retaining, frame = capture.read()\n",
        "            if frame is None:\n",
        "                continue\n",
        "\n",
        "            if count % EXTRACT_FREQUENCY == 0:\n",
        "                if (frame_height != self.resize_height) or (frame_width != self.resize_width):\n",
        "                    frame = cv2.resize(frame, (self.resize_width, self.resize_height))\n",
        "                frame = self.detectP(frame)\n",
        "                cv2.imwrite(filename=os.path.join(save_dir, video_filename, '0000{}.jpg'.format(str(i))), img=frame)\n",
        "                i += 1\n",
        "            count += 1\n",
        "\n",
        "        # Release the VideoCapture once it is no longer needed\n",
        "        capture.release()\n",
        "\n",
        "    mp_pose = mp.solutions.pose\n",
        "    mp_drawing = mp.solutions.drawing_utils\n",
        "    mp_drawing_styles = mp.solutions.drawing_styles\n",
        "\n",
        "    def detectP(self, frame):\n",
        "        with mp_pose.Pose(\n",
        "            static_image_mode=True, min_detection_confidence=0.5, model_complexity=2) as pose:\n",
        "\n",
        "            # Convert the BGR image to RGB and process it with MediaPipe Pose.\n",
        "            results = pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "            # Print nose landmark.\n",
        "            image_hight, image_width, _ = frame.shape\n",
        "            if not results.pose_landmarks:\n",
        "              a =1\n",
        "            # Draw pose landmarks.\n",
        "            annotated_image = frame.copy()\n",
        "            mp_drawing.draw_landmarks(\n",
        "                annotated_image,\n",
        "                results.pose_landmarks,\n",
        "                mp_pose.POSE_CONNECTIONS,\n",
        "                landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
        "\n",
        "        return annotated_image\n",
        "\n",
        "    def randomflip(self, buffer):\n",
        "        \"\"\"Horizontally flip the given image and ground truth randomly with a probability of 0.5.\"\"\"\n",
        "\n",
        "        if np.random.random() < 0.5:\n",
        "            for i, frame in enumerate(buffer):\n",
        "                frame = cv2.flip(buffer[i], flipCode=1)\n",
        "                buffer[i] = cv2.flip(frame, flipCode=1)\n",
        "\n",
        "        return buffer\n",
        "\n",
        "\n",
        "    def normalize(self, buffer):\n",
        "        for i, frame in enumerate(buffer):\n",
        "            frame -= np.array([[[90.0, 98.0, 102.0]]])\n",
        "            buffer[i] = frame\n",
        "\n",
        "        return buffer\n",
        "\n",
        "    def to_tensor(self, buffer):\n",
        "        return buffer.transpose((3, 0, 1, 2))\n",
        "\n",
        "    def load_frames(self, file_dir):\n",
        "        frames = sorted([os.path.join(file_dir, img) for img in os.listdir(file_dir)])\n",
        "        frame_count = len(frames)\n",
        "        buffer = np.empty((frame_count, self.resize_height, self.resize_width, 3), np.dtype('float32'))\n",
        "        for i, frame_name in enumerate(frames):\n",
        "            frame = np.array(cv2.imread(frame_name)).astype(np.float64)\n",
        "            buffer[i] = frame\n",
        "\n",
        "        return buffer\n",
        "\n",
        "    def crop(self, buffer, clip_len, crop_size):\n",
        "        # randomly select time index for temporal jittering\n",
        "        time_index = np.random.randint(buffer.shape[0] - clip_len)\n",
        "\n",
        "        # Randomly select start indices in order to crop the video\n",
        "        height_index = np.random.randint(buffer.shape[1] - crop_size)\n",
        "        width_index = np.random.randint(buffer.shape[2] - crop_size)\n",
        "\n",
        "        # Crop and jitter the video using indexing. The spatial crop is performed on\n",
        "        # the entire array, so each frame is cropped in the same location. The temporal\n",
        "        # jitter takes place via the selection of consecutive frames\n",
        "        buffer = buffer[time_index:time_index + clip_len,\n",
        "                 height_index:height_index + crop_size,\n",
        "                 width_index:width_index + crop_size, :]\n",
        "\n",
        "        return buffer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  '''\n",
        "    train_data = VideoDataset(dataset='/content/gdrive/MyDrive/sample data/ana_data/shortened_data_for_loader', split='train', clip_len=32, preprocess=False)\n",
        "    train_loader = DataLoader(train_data, batch_size=8, shuffle=True, num_workers=0)\n",
        "\n",
        "    for i, sample in enumerate(train_loader):\n",
        "        inputs = sample[0]\n",
        "        labels = sample[1]\n",
        "        print(i,inputs.size())\n",
        "        print(i, labels)\n",
        "\n",
        "        if i == 0:\n",
        "            break\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAGWdEzbhonK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class C3D(nn.Module):\n",
        "    def __init__(self, num_classes, pretrained=False):\n",
        "        super(C3D, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv3d(3, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
        "        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
        "\n",
        "        self.conv2 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
        "        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
        "\n",
        "        self.conv3a = nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
        "        self.conv3b = nn.Conv3d(256, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
        "        self.pool3 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
        "\n",
        "        self.conv4a = nn.Conv3d(256, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
        "        self.conv4b = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
        "        self.pool4 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
        "\n",
        "        self.conv5a = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
        "        self.conv5b = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
        "        self.pool5 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(0, 1, 1))\n",
        "\n",
        "        self.fc6 = nn.Linear(8192, 4096)\n",
        "        self.fc7 = nn.Linear(4096, 4096)\n",
        "        self.fc8 = nn.Linear(4096, num_classes)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.__init_weight()\n",
        "\n",
        "        if pretrained:\n",
        "            self.__load_pretrained_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.relu(self.conv3a(x))\n",
        "        x = self.relu(self.conv3b(x))\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = self.relu(self.conv4a(x))\n",
        "        x = self.relu(self.conv4b(x))\n",
        "        x = self.pool4(x)\n",
        "\n",
        "        x = self.relu(self.conv5a(x))\n",
        "        x = self.relu(self.conv5b(x))\n",
        "        x = self.pool5(x)\n",
        "\n",
        "        x = x.view(-1, 8192)\n",
        "        x = self.relu(self.fc6(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc7(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        logits = self.fc8(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def __load_pretrained_weights(self):\n",
        "        \"\"\"Initialiaze network.\"\"\"\n",
        "        corresp_name = {\n",
        "                        # Conv1\n",
        "                        \"features.0.weight\": \"conv1.weight\",\n",
        "                        \"features.0.bias\": \"conv1.bias\",\n",
        "                        # Conv2\n",
        "                        \"features.3.weight\": \"conv2.weight\",\n",
        "                        \"features.3.bias\": \"conv2.bias\",\n",
        "                        # Conv3a\n",
        "                        \"features.6.weight\": \"conv3a.weight\",\n",
        "                        \"features.6.bias\": \"conv3a.bias\",\n",
        "                        # Conv3b\n",
        "                        \"features.8.weight\": \"conv3b.weight\",\n",
        "                        \"features.8.bias\": \"conv3b.bias\",\n",
        "                        # Conv4a\n",
        "                        \"features.11.weight\": \"conv4a.weight\",\n",
        "                        \"features.11.bias\": \"conv4a.bias\",\n",
        "                        # Conv4b\n",
        "                        \"features.13.weight\": \"conv4b.weight\",\n",
        "                        \"features.13.bias\": \"conv4b.bias\",\n",
        "                        # Conv5a\n",
        "                        \"features.16.weight\": \"conv5a.weight\",\n",
        "                        \"features.16.bias\": \"conv5a.bias\",\n",
        "                         # Conv5b\n",
        "                        \"features.18.weight\": \"conv5b.weight\",\n",
        "                        \"features.18.bias\": \"conv5b.bias\",\n",
        "                        # fc6\n",
        "                        \"classifier.0.weight\": \"fc6.weight\",\n",
        "                        \"classifier.0.bias\": \"fc6.bias\",\n",
        "                        # fc7\n",
        "                        \"classifier.3.weight\": \"fc7.weight\",\n",
        "                        \"classifier.3.bias\": \"fc7.bias\",\n",
        "                        }\n",
        "\n",
        "        p_dict = torch.load('/content/gdrive/MyDrive/sample data/ana_data/newdata2output')\n",
        "        s_dict = self.state_dict()\n",
        "        for name in p_dict:\n",
        "            if name not in corresp_name:\n",
        "                continue\n",
        "            s_dict[corresp_name[name]] = p_dict[name]\n",
        "        self.load_state_dict(s_dict)\n",
        "\n",
        "    def __init_weight(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv3d):\n",
        "                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                # m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                torch.nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, nn.BatchNorm3d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "def get_1x_lr_params(model):\n",
        "    \"\"\"\n",
        "    This generator returns all the parameters for conv and two fc layers of the net.\n",
        "    \"\"\"\n",
        "    b = [model.conv1, model.conv2, model.conv3a, model.conv3b, model.conv4a, model.conv4b,\n",
        "         model.conv5a, model.conv5b, model.fc6, model.fc7]\n",
        "    for i in range(len(b)):\n",
        "        for k in b[i].parameters():\n",
        "            if k.requires_grad:\n",
        "                yield k\n",
        "\n",
        "def get_10x_lr_params(model):\n",
        "    \"\"\"\n",
        "    This generator returns all the parameters for the last fc layer of the net.\n",
        "    \"\"\"\n",
        "    b = [model.fc8]\n",
        "    for j in range(len(b)):\n",
        "        for k in b[j].parameters():\n",
        "            if k.requires_grad:\n",
        "                yield k\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    '''inputs = torch.rand(1, 3, 16, 112, 112)\n",
        "    net = C3D(num_classes=2, pretrained=False)\n",
        "\n",
        "    outputs = net.forward(inputs)\n",
        "    print(outputs.size()) '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvuxHzP8KTLh",
        "outputId": "229e490d-5729-4be5-d593-f2cfe5b480d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device being used: cpu\n",
            "Total params: 78.00M\n",
            "Training model on /content/gdrive/MyDrive/sample data/ana_data/shortened_data_for_loader dataset...\n",
            "Number of train videos: 64\n",
            "Number of val videos: 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of test videos: 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 4/8 [08:29<07:33, 113.28s/it]"
          ]
        }
      ],
      "source": [
        "import timeit\n",
        "from datetime import datetime\n",
        "import socket\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Use GPU if available else revert to CPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device being used:\", device)\n",
        "\n",
        "nEpochs = 50  # Number of epochs for training\n",
        "resume_epoch = 0  # Default is 0, change if want to resume\n",
        "useTest = True # See evolution of the test set when training\n",
        "nTestInterval = 20 # Run on test set every nTestInterval epochs\n",
        "snapshot = 1 # Store a model every snapshot epochs\n",
        "lr = 1e-3 # Learning rate\n",
        "\n",
        "dataset = '/content/gdrive/MyDrive/sample data/ana_data/shortened_data_for_loader'\n",
        "\n",
        "num_classes = 2\n",
        "\n",
        "\n",
        "save_dir_root = '/content/gdrive/MyDrive/sample data/ana_data/'\n",
        "exp_name = save_dir_root.split('/')[-1]\n",
        "#os.path.join(os.path.dirname(os.path.abspath(__file__)))\n",
        "#exp_name = os.path.dirname(os.path.abspath(__file__)).split('/')[-1]\n",
        "\n",
        "\n",
        "\n",
        "def train_model(dataset=dataset, num_classes=num_classes, lr=lr,\n",
        "                num_epochs=nEpochs, save_epoch=snapshot, useTest=useTest, test_interval=nTestInterval):\n",
        "\n",
        "    model = C3D(num_classes=num_classes, pretrained=False)\n",
        "    train_params = [{'params': get_1x_lr_params(model), 'lr': lr},\n",
        "                    {'params': get_10x_lr_params(model), 'lr': lr * 10}]\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()  # standard crossentropy loss for classification\n",
        "    optimizer = optim.SGD(train_params, lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10,\n",
        "                                          gamma=0.1)  # the scheduler divides the lr by 10 every 10 epochs\n",
        "\n",
        "\n",
        "\n",
        "    print('Total params: %.2fM' % (sum(p.numel() for p in model.parameters()) / 1000000.0))\n",
        "    model.to(device)\n",
        "    criterion.to(device)\n",
        "\n",
        "\n",
        "    print('Training model on {} dataset...'.format(dataset))\n",
        "    train_dataloader = DataLoader(VideoDataset(dataset=dataset, split='train',clip_len=16), batch_size=8, shuffle=True, num_workers=0)\n",
        "    val_dataloader   = DataLoader(VideoDataset(dataset=dataset, split='val',  clip_len=16), batch_size=8, num_workers=0)\n",
        "    test_dataloader  = DataLoader(VideoDataset(dataset=dataset, split='test', clip_len=16), batch_size=8, num_workers=0)\n",
        "\n",
        "    trainval_loaders = {'train': train_dataloader, 'val': val_dataloader}\n",
        "    trainval_sizes = {x: len(trainval_loaders[x].dataset) for x in ['train', 'val']}\n",
        "    test_size = len(test_dataloader.dataset)\n",
        "\n",
        "    for epoch in range(resume_epoch, num_epochs):\n",
        "        # each epoch has a training and validation step\n",
        "        for phase in ['train', 'val']:\n",
        "            start_time = timeit.default_timer()\n",
        "\n",
        "            # reset the running loss and corrects\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0.0\n",
        "\n",
        "            # set model to train() or eval() mode depending on whether it is trained\n",
        "            # or being validated. Primarily affects layers such as BatchNorm or Dropout.\n",
        "            if phase == 'train':\n",
        "                # scheduler.step() is to be called once every epoch during training\n",
        "                scheduler.step()\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            for inputs, labels in tqdm(trainval_loaders[phase]):\n",
        "                # move inputs and labels to the device the training is taking place on\n",
        "                inputs = Variable(inputs, requires_grad=True).to(device)\n",
        "                labels = Variable(labels).to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                if phase == 'train':\n",
        "                    outputs = model(inputs)\n",
        "                else:\n",
        "                    with torch.no_grad():\n",
        "                        outputs = model(inputs)\n",
        "\n",
        "                probs = nn.Softmax(dim=1)(outputs)\n",
        "                preds = torch.max(probs, 1)[1]\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / trainval_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / trainval_sizes[phase]\n",
        "\n",
        "            print(\"[{}] Epoch: {}/{} Loss: {} Acc: {}\".format(phase, epoch+1, nEpochs, epoch_loss, epoch_acc))\n",
        "            stop_time = timeit.default_timer()\n",
        "            print(\"Execution time: \" + str(stop_time - start_time) + \"\\n\")\n",
        "\n",
        "\n",
        "        if useTest and epoch % test_interval == (test_interval - 1):\n",
        "            model.eval()\n",
        "            start_time = timeit.default_timer()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0.0\n",
        "\n",
        "            for inputs, labels in tqdm(test_dataloader):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(inputs)\n",
        "                probs = nn.Softmax(dim=1)(outputs)\n",
        "                preds = torch.max(probs, 1)[1]\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / test_size\n",
        "            epoch_acc = running_corrects.double() / test_size\n",
        "\n",
        "            print(\"[test] Epoch: {}/{} Loss: {} Acc: {}\".format(epoch+1, nEpochs, epoch_loss, epoch_acc))\n",
        "            stop_time = timeit.default_timer()\n",
        "            print(\"Execution time: \" + str(stop_time - start_time) + \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHwq5eqqqNIY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}