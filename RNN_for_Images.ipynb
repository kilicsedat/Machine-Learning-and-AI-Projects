{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SY8JF5fQ2o-",
        "outputId": "259451c5-f3e8-4adf-86a5-f84541ad44a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/sample\\ data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8J8qqLU-Q7GT",
        "outputId": "3e5a9bd5-f70e-4b43-9f05-90162deb3fcb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/sample data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "b4TB-XPoRAx_",
        "outputId": "8da72c3e-6cc4-41e4-80eb-3579cc730f3b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/sample data'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.io import read_image\n",
        "from torchvision.transforms import Compose, Resize, Normalize, ToTensor\n",
        "\n",
        "class VideoFrameDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, frames_per_clip=16):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.frames_per_clip = frames_per_clip\n",
        "        self.samples = self._load_samples()\n",
        "\n",
        "    def _load_samples(self):\n",
        "        samples = []\n",
        "        for class_name in os.listdir(self.root_dir):\n",
        "            class_dir = os.path.join(self.root_dir, class_name)\n",
        "            for video_name in os.listdir(class_dir):\n",
        "                video_dir = os.path.join(class_dir, video_name)\n",
        "                frames = sorted(os.listdir(video_dir))\n",
        "                for start_idx in range(0, len(frames) - self.frames_per_clip + 1, self.frames_per_clip):\n",
        "                    # This will create a sample for every sequence of 16 frames,\n",
        "                    # ignoring the last set if it's less than 16 frames.\n",
        "                    end_idx = start_idx + self.frames_per_clip\n",
        "                    if end_idx <= len(frames):\n",
        "                        sample_frames = frames[start_idx:end_idx]\n",
        "                        samples.append((video_dir, class_name, sample_frames))\n",
        "        return samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_dir, class_name, frame_names = self.samples[idx]\n",
        "        frames = [os.path.join(video_dir, frame) for frame in frame_names]\n",
        "        images = [read_image(frame) for frame in frames]\n",
        "        if self.transform:\n",
        "            images = [image.float() / 255. for image in images]\n",
        "            images = [self.transform(image) for image in images]\n",
        "        images = torch.stack(images)\n",
        "        label = 0 if class_name == \"normal\" else 1\n",
        "        return images, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "\n",
        "# Example transform\n",
        "transform = Compose([\n",
        "    Resize((112, 112)),\n",
        "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "dataset = VideoFrameDataset(root_dir='ana_data/data_rnn', transform=transform)\n"
      ],
      "metadata": {
        "id": "hBJow6PfRErb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "for inputs, labels in dataloader:\n",
        "    print(inputs.shape)"
      ],
      "metadata": {
        "id": "ZIF2exktfptx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class VideoClassifierRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VideoClassifierRNN, self).__init__()\n",
        "        # The input size should match the number of features in the flattened image\n",
        "        self.rnn = nn.LSTM(input_size=112*112*3, hidden_size=128, num_layers=3, batch_first=True)\n",
        "        self.fc = nn.Linear(128, 2)  # Assuming 2 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reshape to (batch_size, seq_len, feature_size)\n",
        "        batch_size, seq_len, C, H, W = x.size()\n",
        "        x = x.view(batch_size, seq_len, -1)\n",
        "\n",
        "\n",
        "        # Now x is of shape (batch_size, seq_len, feature_size), which is suitable for LSTM\n",
        "        _, (hn, _) = self.rnn(x)\n",
        "        # Use the last hidden state\n",
        "        x = self.fc(hn[-1])\n",
        "        return x\n",
        "\n",
        "model = VideoClassifierRNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def train(model, dataset, criterion, optimizer, epochs=10, batch_size=4):\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    for epoch in range(epochs):\n",
        "        for inputs, labels in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
        "\n",
        "# Training the model\n",
        "train(model, dataset, criterion, optimizer, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWs3gETOXRk-",
        "outputId": "1147410d-f51b-49e9-af4d-2a5d44f536ec"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.011745582334697247\n",
            "Epoch 2, Loss: 0.01080731675028801\n",
            "Epoch 3, Loss: 0.0007687236065976322\n",
            "Epoch 4, Loss: 0.0005237876321189106\n",
            "Epoch 5, Loss: 0.00038211196078918874\n",
            "Epoch 6, Loss: 0.00029881304362788796\n",
            "Epoch 7, Loss: 0.0002456601650919765\n",
            "Epoch 8, Loss: 0.0002094287920044735\n",
            "Epoch 9, Loss: 0.0001829695247579366\n",
            "Epoch 10, Loss: 0.00016080040950328112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class EnhancedVideoClassifierRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedVideoClassifierRNN, self).__init__()\n",
        "        # The input size should match the number of features in the flattened image.\n",
        "        # Increasing the number of layers, adding bidirectional processing, and introducing dropout.\n",
        "        self.rnn = nn.LSTM(input_size=112*112*3, hidden_size=256, num_layers=4, batch_first=True,\n",
        "                           dropout=0.5, bidirectional=True)\n",
        "\n",
        "        # Adjusting the input size of the fully connected layer to account for bidirectional output\n",
        "        self.fc1 = nn.Linear(256*2, 128)  # *2 for bidirectional\n",
        "        self.bn1 = nn.BatchNorm1d(128)  # Batch Normalization\n",
        "        self.dropout = nn.Dropout(0.5)  # Dropout for regularization\n",
        "        self.fc2 = nn.Linear(128, 2)  # Assuming 2 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Assuming x is of shape (batch_size, seq_len, channels, height, width)\n",
        "        # Reshape to (batch_size, seq_len, feature_size)\n",
        "        batch_size, seq_len, C, H, W = x.size()\n",
        "        #print(\"Before reshaping:\", x.size())\n",
        "        x = x.view(batch_size, seq_len, -1)\n",
        "        #print(\"After reshaping:\", x.size())\n",
        "\n",
        "        # Forward pass through LSTM\n",
        "        # Output shape of rnn_out: (batch_size, seq_len, num_directions * hidden_size)\n",
        "        rnn_out, (hn, cn) = self.rnn(x)\n",
        "\n",
        "        # Using the last output of the last layer (considering bidirectional outputs)\n",
        "        # hn shape: (num_layers * num_directions, batch_size, hidden_size)\n",
        "        x = hn[-2:].transpose(0, 1).contiguous().view(batch_size, -1)  # Reshaping to combine bidirectional outputs\n",
        "\n",
        "        # Forward pass through the fully connected layers\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = EnhancedVideoClassifierRNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def train(model, dataset, criterion, optimizer, epochs=10, batch_size=4):\n",
        "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True, drop_last=True)\n",
        "    for epoch in range(epochs):\n",
        "        for inputs, labels in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
        "\n",
        "# Training the model\n",
        "train(model, dataset, criterion, optimizer, epochs=10)"
      ],
      "metadata": {
        "id": "GOmdPnu5XSdu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ec39e40-4ac5-44bd-ac17-afc0b866425f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.10524152219295502\n",
            "Epoch 2, Loss: 1.0409517288208008\n",
            "Epoch 3, Loss: 1.2412147521972656\n",
            "Epoch 4, Loss: 0.024795599281787872\n",
            "Epoch 5, Loss: 1.0908164978027344\n",
            "Epoch 6, Loss: 0.3986453413963318\n",
            "Epoch 7, Loss: 1.8077200651168823\n",
            "Epoch 8, Loss: 0.09488414227962494\n",
            "Epoch 9, Loss: 0.023501720279455185\n",
            "Epoch 10, Loss: 0.43244320154190063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yjnn8vNYXVZ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}